{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Fake News Challenge\n",
    "\n",
    "http://www.fakenewschallenge.org/\n",
    "\n",
    "https://github.com/FakeNewsChallenge/fnc-1-baseline\n",
    "\n",
    "# 2. Technical Links\n",
    "\n",
    "## NLTK\n",
    "\n",
    "http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization\n",
    "\n",
    "https://www.dataquest.io/blog/natural-language-processing-with-python/\n",
    "\n",
    "http://www.nltk.org/book/ch03.html\n",
    "\n",
    "\n",
    "## Tensorflow\n",
    "https://www.tensorflow.org/tutorials/recurrent\n",
    "\n",
    "https://www.tensorflow.org/programmers_guide/reading_data\n",
    "\n",
    "## Sklearn\n",
    "\n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    "\n",
    "## Markdown\n",
    "\n",
    "https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\n",
    "\n",
    "## Keras\n",
    "\n",
    "https://keras.io/getting-started/sequential-model-guide/\n",
    "\n",
    "# 3. Papers\n",
    "\n",
    "https://www.ijcai.org/Proceedings/16/Papers/408.pdf\n",
    "\n",
    "https://www.overleaf.com/5276203cwvkhf#/16617343/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "FNC_PATH=\"fnc-1-baseline\"\n",
    "\n",
    "#must add local path to the FNC utils, so we can import and reuse them\n",
    "sys.path.append(FNC_PATH + '/utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_data(path=FNC_PATH + '/fnc-1'):\n",
    "    stances = pd.read_csv(path + '/train_stances.csv')\n",
    "    stances.set_index('Body ID', inplace=True)\n",
    "    \n",
    "    bodies = pd.read_csv(path + '/train_bodies.csv')\n",
    "    bodies.set_index('Body ID', inplace=True)\n",
    "    \n",
    "    ds = pd.merge(bodies, stances, how='inner', right_index=True, left_index=True)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_data_split(ds, test_size = 0.2):\n",
    "    train, validation = train_test_split(ds, test_size = test_size)\n",
    "    return train, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ds = read_data()\n",
    "train, validation = get_data_split(ds)\n",
    "print \"Train examples: %d\"%len(train)\n",
    "print \"Test examples: %d\"%len(validation)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import preprocessing\n",
    "import numpy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "le = preprocessing.LabelEncoder()\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3), lowercase=True, stop_words=\"english\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = numpy.arange(num_labels) * num_classes\n",
    "    labels_one_hot = numpy.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "def normalize_word(w):\n",
    "    return wnl.lemmatize(w.lower()).lower()\n",
    "\n",
    "def tokenize_sentenses(sentences):\n",
    "    return sentences.apply(lambda s: nltk.word_tokenize(s.decode('utf-8')))\n",
    "\n",
    "def lemmatize_tokens(series):\n",
    "    return series.apply(lambda tokens: [normalize_word(t) for t in tokens])\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    # Removes stopwords from a list of tokens\n",
    "    return words.apply(lambda l: [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS])\n",
    "\n",
    "def prepare_features(dataset):\n",
    "    #Usefull link https://www.dataquest.io/blog/natural-language-processing-with-python/\n",
    "    #dataset.loc[:, 'Tokens'] = tokenize_sentenses(train['Headline'])\n",
    "    #dataset.loc[:, 'Lemmas'] = lemmatize_tokens(dataset['Tokens'])\n",
    "    #dataset.loc[:, 'StopRemoved'] = remove_stopwords(dataset['Lemmas'])\n",
    "    #dataset.loc[:, 'TFIDF'] = vectorizer.fit_transform(dataset['Headline']).toarray().tolist()\n",
    "    #dataset.loc[:, 'PosTags'] = dataset['Tokens'].apply(lambda x : nltk.pos_tag(x))\n",
    "    \n",
    "    y = dense_to_one_hot(le.fit_transform(train['Stance']), 4)\n",
    "    \n",
    "    return dataset,  vectorizer.fit_transform(dataset['Headline']), y \n",
    "\n",
    "train, matrix, train_labels = prepare_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train):\n",
    "    x_train = x_train.toarray()\n",
    "    input_size = x_train.shape[1]\n",
    "    output_size = 4\n",
    "    \n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, Activation\n",
    "    from keras.optimizers import SGD\n",
    "\n",
    "    model = Sequential()\n",
    "    # Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "    # in the first layer, you must specify the expected input data shape:\n",
    "    # here, 20-dimensional vectors.\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_size))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax'))\n",
    "\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              epochs=20,\n",
    "              batch_size=128)\n",
    "    #score = model.evaluate(x_test, y_test, batch_size=16)\n",
    "    \n",
    "train_model(matrix, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
