{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Fake News Challenge\n",
    "\n",
    "http://www.fakenewschallenge.org/\n",
    "\n",
    "https://github.com/FakeNewsChallenge/fnc-1-baseline\n",
    "\n",
    "# 2. Technical Links\n",
    "\n",
    "## NLTK\n",
    "\n",
    "http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization\n",
    "\n",
    "https://www.dataquest.io/blog/natural-language-processing-with-python/\n",
    "\n",
    "http://www.nltk.org/book/ch03.html\n",
    "\n",
    "\n",
    "## Tensorflow\n",
    "https://www.tensorflow.org/tutorials/recurrent\n",
    "\n",
    "https://www.tensorflow.org/programmers_guide/reading_data\n",
    "\n",
    "## Sklearn\n",
    "\n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    "\n",
    "## Markdown\n",
    "\n",
    "https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\n",
    "\n",
    "## Keras\n",
    "\n",
    "https://keras.io/getting-started/sequential-model-guide/\n",
    "\n",
    "# 3. Papers\n",
    "\n",
    "https://www.ijcai.org/Proceedings/16/Papers/408.pdf\n",
    "\n",
    "https://www.overleaf.com/5276203cwvkhf#/16617343/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "HOME_DIR='.'\n",
    "FNC_PATH='{}/fnc-1-baseline'.format(HOME_DIR)\n",
    "\n",
    "#must add local path to the FNC utils, so we can import and reuse them\n",
    "sys.path.append(FNC_PATH + '/utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "W2V_MODEL='{}/model/GoogleNews-vectors-negative300.bin'.format(HOME_DIR)\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "w2vmodel = gensim.models.KeyedVectors.load_word2vec_format(W2V_MODEL, binary=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_data(path=FNC_PATH + '/fnc-1'):\n",
    "    stances = pd.read_csv(path + '/train_stances.csv')\n",
    "    stances.set_index('Body ID', inplace=True)\n",
    "    \n",
    "    bodies = pd.read_csv(path + '/train_bodies.csv')\n",
    "    bodies.set_index('Body ID', inplace=True)\n",
    "    \n",
    "    ds = pd.merge(bodies, stances, how='inner', right_index=True, left_index=True)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_data_split(ds, test_size = 0.2):\n",
    "    train, validation = train_test_split(ds, test_size = test_size)\n",
    "    return train, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ds = read_data()\n",
    "train, validation = get_data_split(ds)\n",
    "print \"Train examples: %d\"%len(train)\n",
    "print \"Test examples: %d\"%len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = numpy.arange(num_labels) * num_classes\n",
    "    labels_one_hot = numpy.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "def normalize_word(w):\n",
    "    return wnl.lemmatize(w.lower()).lower()\n",
    "\n",
    "def tokenize_sentenses(sentences):\n",
    "    return sentences.apply(lambda s: nltk.word_tokenize(s.decode('utf-8')))\n",
    "\n",
    "def lemmatize_tokens(series):\n",
    "    return series.apply(lambda tokens: [normalize_word(t) for t in tokens])\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    # Removes stopwords from a list of tokens\n",
    "    return words.apply(lambda l: [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS])\n",
    "\n",
    "def trainTFIDF(corpus):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,1), lowercase=True, stop_words=\"english\")\n",
    "    vectorizer.fit(corpus)\n",
    "    \n",
    "    return vectorizer\n",
    "\n",
    "def encode_pos(word):\n",
    "    return ['_'.join(x) for x in nltk.pos_tag(word)]\n",
    "\n",
    "def prepare_features(dataset):\n",
    "    #Usefull link https://www.dataquest.io/blog/natural-language-processing-with-python/\n",
    "    tokens = tokenize_sentenses(dataset['Headline'])\n",
    "    lemmas = lemmatize_tokens(tokens)\n",
    "    no_stop_words = remove_stopwords(lemmas)\n",
    "    pos_tags = no_stop_words.apply(encode_pos)\n",
    "    tf_idf = vectorizer.transform(dataset['Headline'])\n",
    "    #np.asmatrix(matrix.tolist())\n",
    "    embeddings = no_stop_words.apply(lambda x: np.mean([w2vmodel[w] if w in w2vmodel.vocab else np.zeros(300) for w in x], axis=0 ))\n",
    "    \n",
    "    return tf_idf\n",
    "\n",
    "vectorizer = trainTFIDF(train['articleBody'])\n",
    "matrix = prepare_features(train)\n",
    "train_labels = dense_to_one_hot(le.fit_transform(train['Stance']), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train):\n",
    "    x_train = x_train.toarray()\n",
    "    input_size = x_train.shape[1]\n",
    "    output_size = 4\n",
    "    \n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, Activation\n",
    "    from keras.optimizers import SGD\n",
    "\n",
    "    model = Sequential()\n",
    "    # Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "    # in the first layer, you must specify the expected input data shape:\n",
    "    # here, 20-dimensional vectors.\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_size))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax'))\n",
    "\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              epochs=20,\n",
    "              batch_size=128)\n",
    "    #score = model.evaluate(x_test, y_test, batch_size=16)\n",
    "    \n",
    "train_model(matrix, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
