{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fake News Challenge\n",
    "\n",
    "http://www.fakenewschallenge.org/\n",
    "\n",
    "https://github.com/FakeNewsChallenge/fnc-1-baseline\n",
    "\n",
    "# 2. Technical Links\n",
    "\n",
    "## NLTK\n",
    "\n",
    "http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization\n",
    "\n",
    "https://www.dataquest.io/blog/natural-language-processing-with-python/\n",
    "\n",
    "http://www.nltk.org/book/ch03.html\n",
    "\n",
    "\n",
    "## Tensorflow\n",
    "https://www.tensorflow.org/tutorials/recurrent\n",
    "\n",
    "https://www.tensorflow.org/programmers_guide/reading_data\n",
    "\n",
    "## Sklearn\n",
    "\n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    "\n",
    "## Markdown\n",
    "\n",
    "https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\n",
    "\n",
    "# 3. Papers\n",
    "\n",
    "https://www.ijcai.org/Proceedings/16/Papers/408.pdf\n",
    "\n",
    "https://www.overleaf.com/5276203cwvkhf#/16617343/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "#must add local path to the FNC utils, so we can import and reuse them\n",
    "sys.path.append('fnc-1-baseline/utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_data(path='fnc-1-baseline/fnc-1'):\n",
    "    stances = pd.read_csv(path + '/train_stances.csv')\n",
    "    stances.set_index('Body ID', inplace=True)s\n",
    "    \n",
    "    bodies = pd.read_csv(path + '/train_bodies.csv')\n",
    "    bodies.set_index('Body ID', inplace=True)\n",
    "    \n",
    "    ds = pd.merge(bodies, stances, how='inner', right_index=True, left_index=True)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_data_split(ds, test_size = 0.2):\n",
    "    train, validation = train_test_split(ds, test_size = test_size)\n",
    "    return train, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 39977\n",
      "Test examples: 9995\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Body ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>An executive engineer at the Central Public Wo...</td>\n",
       "      <td>Government fires employee who skipped work for...</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>We're just two months away from the Apple Watc...</td>\n",
       "      <td>Video Messaging App Says Audio Recording Of Mi...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>BEIRUT — Islamic State group fighters seized a...</td>\n",
       "      <td>Turkish president says American weapons drop f...</td>\n",
       "      <td>discuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>Call it Newton’s third law of Apple analysts: ...</td>\n",
       "      <td>Web of confusion as scientists cast doubt on m...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Over the weekend, NBC anchor Lester Holt cut t...</td>\n",
       "      <td>Boko Haram 'to release abducted schoolgirls' a...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               articleBody  \\\n",
       "Body ID                                                      \n",
       "1951     An executive engineer at the Central Public Wo...   \n",
       "1808     We're just two months away from the Apple Watc...   \n",
       "2509     BEIRUT — Islamic State group fighters seized a...   \n",
       "1210     Call it Newton’s third law of Apple analysts: ...   \n",
       "320      Over the weekend, NBC anchor Lester Holt cut t...   \n",
       "\n",
       "                                                  Headline     Stance  \n",
       "Body ID                                                                \n",
       "1951     Government fires employee who skipped work for...      agree  \n",
       "1808     Video Messaging App Says Audio Recording Of Mi...  unrelated  \n",
       "2509     Turkish president says American weapons drop f...    discuss  \n",
       "1210     Web of confusion as scientists cast doubt on m...  unrelated  \n",
       "320      Boko Haram 'to release abducted schoolgirls' a...  unrelated  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = read_data()\n",
    "train, validation = get_data_split(ds)\n",
    "print \"Train examples: %d\"%len(train)\n",
    "print \"Test examples: %d\"%len(validation)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import preprocessing\n",
    "import numpy\n",
    "    \n",
    "le = preprocessing.LabelEncoder()\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = numpy.arange(num_labels) * num_classes\n",
    "    labels_one_hot = numpy.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "def normalize_word(w):\n",
    "    return wnl.lemmatize(w.lower()).lower()\n",
    "\n",
    "def tokenize_sentenses(sentences):\n",
    "    return sentences.apply(lambda s: nltk.word_tokenize(s.decode('utf-8')))\n",
    "\n",
    "def lemmatize_tokens(series):\n",
    "    return series.apply(lambda tokens: [normalize_word(t) for t in tokens])\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    # Removes stopwords from a list of tokens\n",
    "    return words.apply(lambda l: [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS])\n",
    "\n",
    "def get_matrix(train):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    vectorizer = CountVectorizer(lowercase=True, stop_words=\"english\")\n",
    "\n",
    "    matrix = vectorizer.fit_transform(train['Headline'])\n",
    "    matrix.todense()\n",
    "\n",
    "def prepare_features(dataset):\n",
    "    #Usefull link https://www.dataquest.io/blog/natural-language-processing-with-python/\n",
    "    dataset.loc[:, 'Tokens'] = tokenize_sentenses(train['Headline'])\n",
    "    dataset.loc[:, 'Lemmas'] = lemmatize_tokens(dataset['Tokens'])\n",
    "    dataset.loc[:, 'StopRemoved'] = remove_stopwords(dataset['Lemmas'])\n",
    "    dataset.loc[:, 'Bigrams'] = nltk.ngrams(dataset['Lemmas'], 2)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "train = prepare_features(train)\n",
    "train_labels = dense_to_one_hot(le.fit_transform(train['Stance']), 4)\n",
    "#matrix = lb.fit_transform(train['StopRemoved'])\n",
    "\n",
    "#Temp solution until we use something that maps index to vector (numpy.array), we can reuse the dense_to_one_hot\n",
    "matrix = get_matrix(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def next_batch(images, labels, batch_size):\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    num_examples = len(images)\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    \n",
    "    # Go to the next epoch\n",
    "    if start + batch_size > num_examples:\n",
    "      # Finished epoch\n",
    "      epochs_completed += 1\n",
    "      # Get the rest examples in this epoch\n",
    "      rest_num_examples = num_examples - start\n",
    "      images_rest_part = images[start:num_examples]\n",
    "      labels_rest_part = labels[start:num_examples]\n",
    "      \n",
    "      # Start next epoch\n",
    "      start = 0\n",
    "      index_in_epoch = batch_size - rest_num_examples\n",
    "      end = index_in_epoch\n",
    "      images_new_part = images[start:end]\n",
    "      labels_new_part = labels[start:end]\n",
    "      return numpy.concatenate((images_rest_part, images_new_part), axis=0) , numpy.concatenate((labels_rest_part, labels_new_part), axis=0)\n",
    "    else:\n",
    "      index_in_epoch += batch_size\n",
    "      end = index_in_epoch\n",
    "    return images[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "    \n",
    "index_in_epoch = 0\n",
    "epochs_completed = 0\n",
    "    \n",
    "    \n",
    "def train_algo(train_news):\n",
    "    \n",
    "    input_size = matrix.shape[1]\n",
    "    output_size = 4\n",
    "    x = tf.placeholder(tf.float32, shape=[None, input_size])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, output_size])\n",
    "\n",
    "    W = tf.Variable(tf.zeros([input_size,output_size]))\n",
    "    b = tf.Variable(tf.zeros([output_size]))\n",
    "\n",
    "    y = tf.matmul(x,W) + b \n",
    "    \n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "    #+ Regularization\n",
    "    #0.01*tf.nn.l2_loss(hidden_weights) +\n",
    "    #0.01*tf.nn.l2_loss(hidden_biases) +\n",
    "    #0.01*tf.nn.l2_loss(out_weights) +\n",
    "    #0.01*tf.nn.l2_loss(out_biases)\n",
    "    \n",
    "    train_step = tf.train.GradientDescentOptimizer(1e-10).minimize(cross_entropy)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for i in range(20000):\n",
    "            batch_xs, batch_ys = next_batch(train_news, train_labels, 500)\n",
    "            \n",
    "            sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "            if i%100 == 0:\n",
    "                train_accuracy = accuracy.eval(feed_dict={\n",
    "                    x:batch_xs, y_: batch_ys})\n",
    "                print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "    #correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    #print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "train_algo(matrix.todense())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
